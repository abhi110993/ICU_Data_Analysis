{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM,CuDNNLSTM\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: 1558  256 Y dimension:1558\n"
     ]
    }
   ],
   "source": [
    "xnormal = []\n",
    "ynormal = []\n",
    "\n",
    "\n",
    "# Reading HeartRate data for Normal Patients\n",
    "#f = \n",
    "#f = \n",
    "#f = \n",
    "f = [open(\"./NormalData/OxygenSaturation.csv\", \"r\"), open(\"./NormalData/HeartRate.csv\", \"r\"), open(\"./NormalData/BloodPressure.csv\", \"r\"), open(\"./NormalData/RespiratoryRate.csv\", \"r\")]\n",
    "\n",
    "\n",
    "s1=f[0].readline()\n",
    "#s1=f[1].readline()\n",
    "#s1=f[2].readline()\n",
    "#s1=f[3].readline()\n",
    "\n",
    "\n",
    "#print(s1)\n",
    "#print(s2)\n",
    "#print(s3)\n",
    "#print(s4)\n",
    "while s1!=\"\":\n",
    "    a = []\n",
    "    b=s1.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "    xnormal.append(a)\n",
    "    ynormal.append(0)\n",
    "    #print(len(a))\n",
    "    s1=f[0].readline()\n",
    "    #s2=f[1].readline()\n",
    "    #s3=f[2].readline()\n",
    "    #s4=f[3].readline()\n",
    "'''    \n",
    "    b=s2.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "    b=s3.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "    b=s4.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "'''\n",
    "print(\"Normal: \"+str(len(xnormal))+\"  \"+str(len(xnormal[0]))+\" Y dimension:\"+str(len(ynormal)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abnormal: 461  256 Y dimension:461\n"
     ]
    }
   ],
   "source": [
    "f = [open(\"./AbnormalData/OxygenSaturation.csv\", \"r\"), open(\"./AbnormalData/HeartRate.csv\", \"r\"), open(\"./AbnormalData/BloodPressure.csv\", \"r\"), open(\"./AbnormalData/RespiratoryRate.csv\", \"r\")]\n",
    "\n",
    "xAbnormal=[]\n",
    "yAbnormal=[]\n",
    "\n",
    "s1=f[0].readline()\n",
    "#s1=f[1].readline()\n",
    "#s1=f[2].readline()\n",
    "#s1=f[3].readline()\n",
    "\n",
    "\n",
    "#print(s1)\n",
    "#print(s2)\n",
    "#print(s3)\n",
    "#print(s4)\n",
    "while s1!=\"\":\n",
    "    a = []\n",
    "    b=s1.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "        #print(b[i])\n",
    "        #print(\"a=\"+str(a))\n",
    "\n",
    "    xAbnormal.append(a)\n",
    "    yAbnormal.append(1)\n",
    "    #print(len(a))\n",
    "    s1=f[0].readline()\n",
    "    #s2=f[1].readline()\n",
    "    #s3=f[2].readline()\n",
    "    #s4=f[3].readline()\n",
    "'''    b=s2.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "    b=s3.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "    b=s4.split(',')\n",
    "    for i in range(0, len(b)):\n",
    "        a.append(float(b[i]))\n",
    "'''\n",
    "\n",
    "print(\"Abnormal: \"+str(len(xAbnormal))+\"  \"+str(len(xAbnormal[0]))+\" Y dimension:\"+str(len(yAbnormal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461  256 Y dimension:461\n",
      "921  256 Y dimension:921\n"
     ]
    }
   ],
   "source": [
    "cTrainx=[] # current train x data\n",
    "cTrainy=[] # current test y data\n",
    "\n",
    "for i in range(0, 461):\n",
    "    cTrainx.append(xnormal[i])\n",
    "    cTrainy.append(ynormal[i])\n",
    "print(str(len(cTrainx))+\"  \"+str(len(cTrainx[0]))+\" Y dimension:\"+str(len(cTrainy)))\n",
    "i=0\n",
    "for i in range(0, 460):\n",
    "    cTrainx.append(xAbnormal[i])\n",
    "    cTrainy.append(yAbnormal[i])\n",
    "print(str(len(cTrainx))+\"  \"+str(len(cTrainx[0]))+\" Y dimension:\"+str(len(cTrainy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 256)\n",
      "(921, 256, 1)\n",
      "(921,)\n"
     ]
    }
   ],
   "source": [
    "# List to numpy array conversion\n",
    "y=np.transpose(cTrainy)\n",
    "x = np.array(cTrainx)\n",
    "print(x.shape)\n",
    "x = x.reshape(len(x),len(x[0]),1);\n",
    "print(x.shape)\n",
    "#print(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 256, 1)\n",
      "(736,)\n",
      "[1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1\n",
      " 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
      " 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1\n",
      " 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0\n",
      " 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0\n",
      " 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1\n",
      " 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1\n",
      " 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0\n",
      " 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1]\n",
      "Total normal data after distribution: 381\n",
      "Total Abnormal data: 355\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=50)\n",
    "x_train = tf.keras.utils.normalize(X_train, axis=0)\n",
    "x_test = tf.keras.utils.normalize(X_test, axis=0)\n",
    "print(x_train.shape)\n",
    "#print(x_train)\n",
    "#y_train=np.array([1,1])\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "count=0\n",
    "for op in y_train:\n",
    "    if(op==1):\n",
    "        count+=1\n",
    "print(\"Total normal data after distribution: \"+str(len(y_train)-count)+\"\\nTotal Abnormal data: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 256, 1)\n",
      "(256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(32, input_shape=(x_train.shape[1:]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(CuDNNLSTM(32))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 736 samples, validate on 185 samples\n",
      "Epoch 1/20\n",
      "736/736 [==============================] - 0s 542us/sample - loss: 0.7454 - acc: 0.5095 - val_loss: 0.6843 - val_acc: 0.5676\n",
      "Epoch 2/20\n",
      "736/736 [==============================] - 0s 466us/sample - loss: 0.7053 - acc: 0.4851 - val_loss: 0.6892 - val_acc: 0.5676\n",
      "Epoch 3/20\n",
      "736/736 [==============================] - 0s 443us/sample - loss: 0.7400 - acc: 0.5014 - val_loss: 0.7952 - val_acc: 0.5676\n",
      "Epoch 4/20\n",
      "736/736 [==============================] - 0s 453us/sample - loss: 0.7511 - acc: 0.4715 - val_loss: 0.7111 - val_acc: 0.4324\n",
      "Epoch 5/20\n",
      "736/736 [==============================] - 0s 470us/sample - loss: 0.7053 - acc: 0.5095 - val_loss: 0.7197 - val_acc: 0.4324\n",
      "Epoch 6/20\n",
      "736/736 [==============================] - 0s 541us/sample - loss: 0.7600 - acc: 0.5041 - val_loss: 0.6848 - val_acc: 0.5676\n",
      "Epoch 7/20\n",
      "736/736 [==============================] - 0s 534us/sample - loss: 0.7646 - acc: 0.4851 - val_loss: 0.7079 - val_acc: 0.5676\n",
      "Epoch 8/20\n",
      "736/736 [==============================] - 0s 493us/sample - loss: 0.7226 - acc: 0.5041 - val_loss: 0.6990 - val_acc: 0.4324\n",
      "Epoch 9/20\n",
      "736/736 [==============================] - 0s 451us/sample - loss: 0.6975 - acc: 0.5122 - val_loss: 0.7495 - val_acc: 0.4324\n",
      "Epoch 10/20\n",
      "736/736 [==============================] - 0s 454us/sample - loss: 0.7141 - acc: 0.5041 - val_loss: 0.7201 - val_acc: 0.4324\n",
      "Epoch 11/20\n",
      "736/736 [==============================] - 0s 451us/sample - loss: 0.7193 - acc: 0.4633 - val_loss: 0.7639 - val_acc: 0.4324\n",
      "Epoch 12/20\n",
      "736/736 [==============================] - 0s 458us/sample - loss: 0.6967 - acc: 0.5149 - val_loss: 0.6840 - val_acc: 0.5676\n",
      "Epoch 13/20\n",
      "736/736 [==============================] - 0s 468us/sample - loss: 0.7197 - acc: 0.4823 - val_loss: 0.6860 - val_acc: 0.5676\n",
      "Epoch 14/20\n",
      "736/736 [==============================] - 0s 467us/sample - loss: 0.7272 - acc: 0.5014 - val_loss: 0.6842 - val_acc: 0.5676\n",
      "Epoch 15/20\n",
      "736/736 [==============================] - 0s 457us/sample - loss: 0.7110 - acc: 0.4986 - val_loss: 0.6842 - val_acc: 0.5676\n",
      "Epoch 16/20\n",
      "736/736 [==============================] - 0s 454us/sample - loss: 0.7141 - acc: 0.5095 - val_loss: 0.7889 - val_acc: 0.4324\n",
      "Epoch 17/20\n",
      "736/736 [==============================] - 0s 453us/sample - loss: 0.7381 - acc: 0.4959 - val_loss: 0.7023 - val_acc: 0.5676\n",
      "Epoch 18/20\n",
      "736/736 [==============================] - 0s 458us/sample - loss: 0.7075 - acc: 0.5014 - val_loss: 0.6845 - val_acc: 0.5676\n",
      "Epoch 19/20\n",
      "736/736 [==============================] - 0s 457us/sample - loss: 0.7158 - acc: 0.4959 - val_loss: 0.7883 - val_acc: 0.4324\n",
      "Epoch 20/20\n",
      "736/736 [==============================] - 0s 461us/sample - loss: 0.7041 - acc: 0.4932 - val_loss: 0.6983 - val_acc: 0.4324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c190bba0f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461/461 [==============================] - 0s 281us/sample - loss: 0.7271 - acc: 0.4729\n",
      "0.7270611147042727\n",
      "0.47288504\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
