{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import plot_model\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558  256 Y dimension:1558\n"
     ]
    }
   ],
   "source": [
    "xnormal = []\n",
    "ynormal = []\n",
    "\n",
    "\n",
    "# Reading HeartRate data for Normal Patients\n",
    "#f = open(\"./NormalData/OxygenSaturation.csv\", \"r\")\n",
    "#f = open(\"./NormalData/HeartRate.csv\", \"r\")\n",
    "#f = open(\"./NormalData/BloodPressure.csv\", \"r\")\n",
    "f = open(\"./NormalData/RespiratoryRate.csv\", \"r\")\n",
    "\n",
    "s=f.readline()\n",
    "while s!=\"\":\n",
    "    a = s.split(',')\n",
    "    for i in range(0, len(a)): \n",
    "        a[i] = float(a[i]) \n",
    "    xnormal.append(a)\n",
    "    ynormal.append(0)\n",
    "    s=f.readline()\n",
    "\n",
    "    \n",
    "print(str(len(xnormal))+\"  \"+str(len(xnormal[0]))+\" Y dimension:\"+str(len(ynormal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461  256 Y dimension:461\n"
     ]
    }
   ],
   "source": [
    "# Reading HeartRate data for Abnormal Patients\n",
    "#f = open(\"./AbnormalData/OxygenSaturation.csv\", \"r\")\n",
    "#f = open(\"./AbnormalData/HeartRate.csv\", \"r\")\n",
    "#f = open(\"./AbnormalData/BloodPressure.csv\", \"r\")\n",
    "f = open(\"./AbnormalData/RespiratoryRate.csv\", \"r\")\n",
    "s=f.readline()\n",
    "xAbnormal=[]\n",
    "yAbnormal=[]\n",
    "while s!=\"\":\n",
    "    a = s.split(',')\n",
    "    for i in range(0, len(a)): \n",
    "        a[i] = float(a[i]) \n",
    "    xAbnormal.append(a)\n",
    "    yAbnormal.append(1)\n",
    "    s=f.readline()\n",
    "print(str(len(xAbnormal))+\"  \"+str(len(xAbnormal[0]))+\" Y dimension:\"+str(len(yAbnormal)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389  256 Y dimension:389\n",
      "849  256 Y dimension:849\n"
     ]
    }
   ],
   "source": [
    "cTrainx=[] # current train x data\n",
    "cTrainy=[] # current test y data\n",
    "\n",
    "for i in range(0, 389):\n",
    "    cTrainx.append(xnormal[i])\n",
    "    cTrainy.append(ynormal[i])\n",
    "print(str(len(cTrainx))+\"  \"+str(len(cTrainx[0]))+\" Y dimension:\"+str(len(cTrainy)))\n",
    "i=0\n",
    "for i in range(0, 460):\n",
    "    cTrainx.append(xAbnormal[i])\n",
    "    cTrainy.append(yAbnormal[i])\n",
    "print(str(len(cTrainx))+\"  \"+str(len(cTrainx[0]))+\" Y dimension:\"+str(len(cTrainy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(849, 256)\n",
      "(849, 1, 256)\n",
      "[[[19.  21.  20.  ... 19.  18.5 16. ]]\n",
      "\n",
      " [[13.  12.  13.5 ... 11.  11.  11. ]]\n",
      "\n",
      " [[14.  12.  15.  ... 14.5 17.  23. ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[20.  23.  16.  ... 23.  15.  17. ]]\n",
      "\n",
      " [[23.  23.  21.  ... 28.  26.  26. ]]\n",
      "\n",
      " [[30.  25.5 33.  ... 20.  21.  11. ]]]\n",
      "(849,)\n"
     ]
    }
   ],
   "source": [
    "# List to numpy array conversion\n",
    "y=np.transpose(cTrainy)\n",
    "x = np.array(cTrainx)\n",
    "print(x.shape)\n",
    "x = x.reshape(len(x),1,len(x[0]));\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 1, 256)\n",
      "(424,)\n",
      "[1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1\n",
      " 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1\n",
      " 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1\n",
      " 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1\n",
      " 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1\n",
      " 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
      " 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1]\n",
      "Total normal data after distribution: 198\n",
      "Total Abnormal data: 226\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.50, random_state=50)\n",
    "x_train = tf.keras.utils.normalize(X_train, axis=0)\n",
    "x_test = tf.keras.utils.normalize(X_test, axis=0)\n",
    "print(x_train.shape)\n",
    "#print(x_train)\n",
    "#y_train=np.array([1,1])\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "count=0\n",
    "for op in y_train:\n",
    "    if(op==1):\n",
    "        count+=1\n",
    "print(\"Total normal data after distribution: \"+str(len(y_train)-count)+\"\\nTotal Abnormal data: \"+str(count))\n",
    "#print(x_test.shape)\n",
    "#print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First layer\n",
    "#model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
    "#Second Layer\n",
    "#model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
    "# tf.nn.sigmoid, tanh, softmax\n",
    "'''\n",
    "for i in range(0,20):\n",
    "    model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))'''\n",
    "\n",
    "#Output Layer\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "# 2 no. of outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "424/424 [==============================] - 0s 216us/sample - loss: 0.6933 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "424/424 [==============================] - 0s 64us/sample - loss: 0.6932 - acc: 0.5000\n",
      "Epoch 3/5\n",
      "424/424 [==============================] - 0s 73us/sample - loss: 0.6932 - acc: 0.5000\n",
      "Epoch 4/5\n",
      "424/424 [==============================] - 0s 66us/sample - loss: 0.6932 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "424/424 [==============================] - 0s 75us/sample - loss: 0.6932 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d8580f748>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425/425 [==============================] - 0s 115us/sample - loss: 0.6932 - acc: 0.5000\n",
      "0.6931708990826326\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
